# Default global configuration for all nodes
---

###########################
## general configuration ##
###########################

# indicate the os architecture (in event that script extended to support beyond intel x64)
# not in use at this stage (assume is amd64)
host_arch: amd64

# indicate wether nodes have access to internet (hence uses public yum repository, etc)
# not in use at this stage (assume is online)
is_online: yes

# force ansible inventory hostname to be the actual node's hostname
ansible_hostname_as_actual: no

# flag to use static hosts file or relies on external_dns (e.g. using vagranthostmanager plugin)
external_dns: yes

# flag to disable the existing local firewall service
disable_firewall: yes

# the list of users to create with support for passwordless login within the cluster
passwordless_ssh_users:
  - name: root
    sudoer: no
    passwordless: yes
    home_dir: /root
  - name: admin
    sudoer: yes
    passwordless: yes

##################################
## hadoop cluster configuration ##
##################################

# name of the hadoop cluster (managed by ambari)
cluster_name: nydoop

# version of ambari to use, must be the 4-part full version number
ambari_version: 2.6.2.2

# version of hadoop stack to use, must be the 4-part full version number
hdp_version: 2.6.5.0
hdp_build_number: 292
hdp_stack_version: "{{ hdp_version }}-{{ hdp_build_number }}" # 2.6.5.0-292

repo_base_url: 'http://public-repo-1.hortonworks.com'     # change this if using a Local Repository

##########################
## ambari configuration ##
##########################

# the default password for all ambari and hadoop component if they are not specified
default_password: admin

# user to access and configure ambari
ambari_admin_user: admin
ambari_admin_password: "{{ default_password }}"

# flag to wait for the cluster to finish installing when executing ansible
wait: true

# The amount of time to wait (in secs) for cluster installation before declaring as failure
wait_timeout: 3600

# set to yes to allow ambari to install GPL licensed libraries
accept_gpl: yes

####################################
## ambari blueprint configuration ##
####################################

# name of the ambari blueprint to deploy
blueprint_name: '{{ cluster_name }}_blueprint'

# the blueprint file (in JSON) to use - Jinja2 template that generates the required JSON
# could use this link to generate the blueprint: http://yabbletech.com/projects/nodeview/html/#/
blueprint_file: 'blueprint.json.j2'

# host mapping file (in JSON) to use - Jinja2 template that generates the required JSON
host_mapping_file: 'host_mapping.json.j2'

# confinguration on the components specified in the ambari blueprint
# namenode_groups:
#   - master_1
#   - master_2
# zookeeper_groups:
#   - cc

blueprint_dynamic:

  - host_group: "master_1"
    clients: ['ZOOKEEPER_CLIENT', 'HDFS_CLIENT', 'YARN_CLIENT', 'MAPREDUCE2_CLIENT', 'TEZ_CLIENT', 'PIG', 'SQOOP', 'HIVE_CLIENT', 'HCAT', 'OOZIE_CLIENT', 'SPARK2_CLIENT']
    services:
      - ZOOKEEPER_SERVER
      - NAMENODE
      - ZKFC
      - JOURNALNODE
      - RESOURCEMANAGER
      - ZEPPELIN_MASTER
      - METRICS_MONITOR

  - host_group: "master_2"
    clients: ['ZOOKEEPER_CLIENT', 'HDFS_CLIENT', 'YARN_CLIENT', 'MAPREDUCE2_CLIENT', 'TEZ_CLIENT', 'PIG', 'SQOOP', 'HIVE_CLIENT', 'OOZIE_CLIENT', 'SPARK2_CLIENT']
    services:
      - ZOOKEEPER_SERVER
      - NAMENODE
      - ZKFC
      - JOURNALNODE
      - RESOURCEMANAGER
      - APP_TIMELINE_SERVER
      - HISTORYSERVER
      - SPARK2_JOBHISTORYSERVER
      - METRICS_MONITOR

  - host_group: "slave_1"
    clients: ['ZOOKEEPER_CLIENT', 'HDFS_CLIENT', 'YARN_CLIENT', 'MAPREDUCE2_CLIENT', 'TEZ_CLIENT', 'PIG', 'SQOOP', 'HIVE_CLIENT', 'HCAT', 'OOZIE_CLIENT', 'SPARK2_CLIENT']
    services:
      - ZOOKEEPER_SERVER
      - JOURNALNODE
      - DATANODE
      - NODEMANAGER
      - METRICS_MONITOR
      - OOZIE_SERVER
      - MYSQL_SERVER
      - HIVE_SERVER
      - HIVE_METASTORE
      - WEBHCAT_SERVER

  - host_group: "slave_2"
    clients: ['HDFS_CLIENT', 'YARN_CLIENT', 'MAPREDUCE2_CLIENT', 'TEZ_CLIENT', 'PIG', 'SQOOP', 'HIVE_CLIENT', 'OOZIE_CLIENT', 'SPARK2_CLIENT']
    services:
      - DATANODE
      - NODEMANAGER
      - METRICS_MONITOR

  - host_group: "slave_3"
    clients: ['HDFS_CLIENT', 'YARN_CLIENT', 'MAPREDUCE2_CLIENT', 'TEZ_CLIENT', 'PIG', 'SQOOP', 'HIVE_CLIENT', 'OOZIE_CLIENT', 'SPARK2_CLIENT']
    services:
      - DATANODE
      - NODEMANAGER
      - METRICS_MONITOR
      - METRICS_COLLECTOR

####################################
## Hadoop cluster configuration ##
####################################

hdfs_data_dir: '/hadoop/hdfs/data'
hdfs_data_failed_volumes_tolerated: 0

hdfs_data_replication: 3

#######################################
## python and pyspark configuration  ##
#######################################

nltk_data_dir: '/usr/share/nltk_data'

nltk_data_package:
  - popular
  - punkt
  - stopwords
  - wordnet

####################################
## EDW configuration ##
####################################

edw_database_name: nydoop
edw_database_user: edw_user
edw_database_password: password

########################
## java configuration ##
########################

java: 'oraclejdk'                                         # can be set to 'embedded' or 'oraclejdk'
oraclejdk_options:                                        # only used when java is set to 'oraclejdk'
  base_folder: '/usr/java'                                # the folder where the Java package should be unpacked to
  tarball_location: '/tmp/jdk-8u112-linux-x64.tar.gz'     # the location of the tarball on the remote system or on the Ansible controller
  jce_location: '/tmp/jce_policy-8.zip'                   # the location of the JCE package on the remote system or on the Ansible controller
  jce_policy_download_url: http://public-repo-1.hortonworks.com/ARTIFACTS/jce_policy-8.zip
  download_url: http://public-repo-1.hortonworks.com/ARTIFACTS/jdk-8u112-linux-x64.tar.gz

############################
## helper variables       ##
############################

# don't change these unless there is a good reason
hdp_minor_version: "{{ hdp_version | regex_replace('.[0-9]+.[0-9]+[0-9_-]*$','') }}"
hdp_major_version: "{{ hdp_minor_version.split('.').0 }}"
utils_version: "{{ '1.1.0.20' if hdp_minor_version is version_compare('2.5', '<') else ('1.1.0.21' if hdp_version is version_compare('2.6.4', '<') else '1.1.0.22' ) }}"
hdfs_ha_name: "{{ cluster_name | regex_replace('_','-') }}"
